{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision Tree from Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Vt_2uPbsppBb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At each node, a decision tree must find out the best split feature and it's best split value\n",
        "After being trained using the training data when a new test observation arrives, a decision tree traverses through the decision tree made from the training data to find the predicted label/class of the test observation."
      ],
      "metadata": {
        "id": "4bTNfn-gqebT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree is a greedy search algorithm because it goes over all the possible features for all the values/thresholds.\n",
        "Then the best features and threshold is chosen"
      ],
      "metadata": {
        "id": "Yc3woSVnsY0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Algorithm:\n",
        "1. Start at the top node and at each node select the best split on the best information gain.\n",
        "2. Greedy Search: Loop over all features and over all thresholds(all possible feature values)\n",
        "3. Save the best split feature and split threshold at each node\n",
        "4. Build the tree recursively\n",
        "5. Apply some stopping criteria to stop growing. Ex: maximum depth, minimum samples at node, no more class distribution in node\n",
        "6. When we have a leaf node, store the most common class label at this node\n",
        "\n",
        "Predict Algorithm:\n",
        "1. Traverse the tree recursively\n",
        "2. At each node look at the best split feature of the test feature vector x and go left or right(based on x[feature_idx]<=threshold\n",
        "3. When we reach the leaf node we return the most common class label"
      ],
      "metadata": {
        "id": "5clWknlTssrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision at each node is made using certain criteria namely:\n",
        "1. Entropy: Entropy = 0 is the best possible case\n",
        "    \n",
        "    E = -âˆ‘p(X).ln(p(X)) where p(X)=#X/n\n",
        "\n",
        "2. Information Gain:\n",
        "\n",
        "    IG = E(parent)-[weighted average]*E(children)"
      ],
      "metadata": {
        "id": "nZKhwH6brRyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node():\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
        "        ''' constructor ''' \n",
        "        \n",
        "        # for decision node\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.info_gain = info_gain\n",
        "        \n",
        "        # for leaf node\n",
        "        self.value = value\n",
        "class DecisionTreeClassifier():\n",
        "    def __init__(self, min_samples_split=2, max_depth=2):\n",
        "        ''' constructor '''\n",
        "        \n",
        "        # initialize the root of the tree \n",
        "        self.root = None\n",
        "        \n",
        "        # stopping conditions\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        \n",
        "    def build_tree(self, dataset, curr_depth=0):\n",
        "        ''' recursive function to build the tree ''' \n",
        "        \n",
        "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
        "        num_samples, num_features = np.shape(X)\n",
        "        \n",
        "        # split until stopping conditions are met\n",
        "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
        "            # find the best split\n",
        "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
        "            # check if information gain is positive\n",
        "            if best_split[\"info_gain\"]>0:\n",
        "                # recur left\n",
        "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
        "                # recur right\n",
        "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
        "                # return decision node\n",
        "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
        "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
        "        \n",
        "        # compute leaf node\n",
        "        leaf_value = self.calculate_leaf_value(Y)\n",
        "        # return leaf node\n",
        "        return Node(value=leaf_value)\n",
        "    \n",
        "    def get_best_split(self, dataset, num_samples, num_features):\n",
        "        ''' function to find the best split '''\n",
        "        \n",
        "        # dictionary to store the best split\n",
        "        best_split = {}\n",
        "        max_info_gain = -float(\"inf\")\n",
        "        \n",
        "        # loop over all the features\n",
        "        for feature_index in range(num_features):\n",
        "            feature_values = dataset[:, feature_index]\n",
        "            possible_thresholds = np.unique(feature_values)\n",
        "            # loop over all the feature values present in the data\n",
        "            for threshold in possible_thresholds:\n",
        "                # get current split\n",
        "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
        "                # check if childs are not null\n",
        "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
        "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
        "                    # compute information gain\n",
        "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
        "                    # update the best split if needed\n",
        "                    if curr_info_gain>max_info_gain:\n",
        "                        best_split[\"feature_index\"] = feature_index\n",
        "                        best_split[\"threshold\"] = threshold\n",
        "                        best_split[\"dataset_left\"] = dataset_left\n",
        "                        best_split[\"dataset_right\"] = dataset_right\n",
        "                        best_split[\"info_gain\"] = curr_info_gain\n",
        "                        max_info_gain = curr_info_gain\n",
        "                        \n",
        "        # return best split\n",
        "        return best_split\n",
        "    \n",
        "    def split(self, dataset, feature_index, threshold):\n",
        "        ''' function to split the data '''\n",
        "        \n",
        "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
        "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
        "        return dataset_left, dataset_right\n",
        "    \n",
        "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
        "        ''' function to compute information gain '''\n",
        "        \n",
        "        weight_l = len(l_child) / len(parent)\n",
        "        weight_r = len(r_child) / len(parent)\n",
        "        if mode==\"gini\":\n",
        "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
        "        else:\n",
        "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
        "        return gain\n",
        "    \n",
        "    def entropy(self, y):\n",
        "        ''' function to compute entropy '''\n",
        "        \n",
        "        class_labels = np.unique(y)\n",
        "        entropy = 0\n",
        "        for cls in class_labels:\n",
        "            p_cls = len(y[y == cls]) / len(y)\n",
        "            entropy += -p_cls * np.log2(p_cls)\n",
        "        return entropy\n",
        "    \n",
        "    def gini_index(self, y):\n",
        "        ''' function to compute gini index '''\n",
        "        \n",
        "        class_labels = np.unique(y)\n",
        "        gini = 0\n",
        "        for cls in class_labels:\n",
        "            p_cls = len(y[y == cls]) / len(y)\n",
        "            gini += p_cls**2\n",
        "        return 1 - gini\n",
        "        \n",
        "    def calculate_leaf_value(self, Y):\n",
        "        ''' function to compute leaf node '''\n",
        "        \n",
        "        Y = list(Y)\n",
        "        return max(Y, key=Y.count)\n",
        "    \n",
        "    def print_tree(self, tree=None, indent=\" \"):\n",
        "        ''' function to print the tree '''\n",
        "        \n",
        "        if not tree:\n",
        "            tree = self.root\n",
        "\n",
        "        if tree.value is not None:\n",
        "            print(tree.value)\n",
        "\n",
        "        else:\n",
        "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
        "            print(\"%sleft:\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.left, indent + indent)\n",
        "            print(\"%sright:\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.right, indent + indent)\n",
        "    \n",
        "    def fit(self, X, Y):\n",
        "        ''' function to train the tree '''\n",
        "        \n",
        "        dataset = np.concatenate((X, Y), axis=1)\n",
        "        self.root = self.build_tree(dataset)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        ''' function to predict new dataset '''\n",
        "        \n",
        "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
        "        return preditions\n",
        "    \n",
        "    def make_prediction(self, x, tree):\n",
        "        ''' function to predict a single data point '''\n",
        "        \n",
        "        if tree.value!=None: return tree.value\n",
        "        feature_val = x[tree.feature_index]\n",
        "        if feature_val<=tree.threshold:\n",
        "            return self.make_prediction(x, tree.left)\n",
        "        else:\n",
        "            return self.make_prediction(x, tree.right)"
      ],
      "metadata": {
        "id": "ATAbmmfLrRNX"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "churn = pd.read_csv('https://raw.githubusercontent.com/Halaarav/Some-Projects/main/churn.csv')"
      ],
      "metadata": {
        "id": "EMFjJY4Q-6j2"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "churn = churn.drop(columns=['RowNumber','CustomerId','Surname','Geography','Gender','Age'])"
      ],
      "metadata": {
        "id": "8qNZ8Ie_--2Z"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = churn.iloc[:, :-1].values\n",
        "y = churn.iloc[:, -1].values.reshape(-1,1)"
      ],
      "metadata": {
        "id": "F1DdKhUF_Bfq"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)"
      ],
      "metadata": {
        "id": "PPCY45N9_EPT"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(max_depth=10)"
      ],
      "metadata": {
        "id": "ombpC-S0_ERZ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "_QUxrGon_ETw"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = dt.predict(X_test)"
      ],
      "metadata": {
        "id": "1Sv6mIXaWEE4"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "dUdWr4U6XezW"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7HBMJ8nXkNI",
        "outputId": "01cdec7e-8112-4fa8-f39d-da359ede6c67"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8108"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    }
  ]
}